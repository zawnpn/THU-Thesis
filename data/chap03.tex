% !TeX root = ../main.tex

\chapter{基于双层缓存的优先经验重放算法}

\section{强化学习中的经验回放}

在第\ref{chap:intro}章中我们已经介绍，尽管使用随机抽取经验样本进行回放的做法能够对算法样本效率起到一定的优化，但是仍然较为局限。而PER算法在样本池中按优先级进行经验回放的做法则更加有效，能够给更重要的样本赋予更高的优先级，确保更有价值的经验得到更多的回放，从而起到了提升样本效率的效果。在PER算法中，理想的设计方案是计算样本池中的样本被用于学习的次数，次数越少，说明该样本越应被优先提供给智能体进行学习。考虑到该指标在实际运行时难以获取，选择了使用时序差分误差来辅助计算样本池中的采样优先级作为替代方案，这是因为时序差分误差大的样本，意味着策略在该样本所处状态上的决策效果存在较大改进空间，同样可以被认为是更有高学习价值的经验样本，因此需要为其分配更高的优先级进行学习。

优先经验回放方法已经在强化学习中取得了很好的效果，但它仍有很多局限性:

\begin{enumerate}
    \item 为避免扫描整个样本回放池的昂贵成本，只有被回放过的经验样本的时序差分误差值会被更新，这将导致一些有较小的初始误差值样本长时间不被回放，进而对整个样本空间覆盖率不高
    \item 时序差分误差对噪声非常敏感，很容易因噪声而增加估计误差，同时较高的时序差分误差带来的频繁回放会因损失多样性而导致算法过拟合
    \item PER只对由访问过的状态组成的回放样本池进行优先级分配。与整个状态空间相比，样本池可能只覆盖其中一个小的子集，实际中并不能实现全局意义上的优先级分配就无法实现，与其设计理念相差甚远。
\end{enumerate}

这些问题在现有的工作中并没有得到太多关注，为此，我们设计了基于双层缓存的优先经验回放算法，可以有效解决上述提到的优先经验回放算法的局限性，学习更多的重要经验，提升强化学习的样本效率效率。

\section{双层经验样本回放池设计}

在优先经验回放算法的基础上，我们设计了一个双层经验回放池。我们在通常的经验重放方法的基础上增加了第二层重放缓冲区，并从经验池中实时提取样本，并将其加入第二层重放缓冲区。然后，这两个缓冲区中的样本被混合起来进行策略优化。我们表明，我们的方法确实提供了接近理想的优先采样分布。我们进行了实验来证明我们方法的有效性。


% In this paper, we design a double layer experience replay mechanism to solve the limitations mentioned above. Like the usual experience replay approach, it first stores the observed samples in the
% experience replay pool. But the difference is that  a second replay buffer is added afterwards for prioritizing the samples. The motivation for this is that we consider that humans do not fully prioritize all experiences when learning from experience, but only partially select important experiences to be learned first. We also set parameters to determine the size of the second layer buffer to dynamically adjust the degree of priority experience replay.

在本文中，我们设计了一种双层经验重放机制来解决上述的限制。与通常的经验重放方法一样，它首先将观察到的样本存储在
经验重放池。但不同的是，之后增加了第二个重放缓冲区，以确定样本的优先次序。这样做的动机是，我们认为人类在从经验中学习时，不会完全优先考虑所有的经验，而只是部分地选择重要的经验来先学习。我们还设置了一些参数来决定第二层缓冲区的大小，以动态地调整优先经验重放的程度。


% In this subsection, we will introduce the Double Layer Experience Replay method and how it can help improve learning efficiency of Reinforcement Learning.

% We firstly train a model based on the environment data, i.e., $\mathcal{D}_{\text{env}}$. After obtaining the model, we first generate a certain amount of samples to fill in the multi-layer batch, first take out hypothetical samples with large deviations for learning, and then directly give priority to infer and learn in the direction that may have large deviations to make up for the shortcomings of the agent policy, followed by regular learning. With the idea of Dyna-style RL, we continually transmit the collected real samples to the model for further optimization in the policy optimization cycle, which enables the model to provide simulation samples for the framework more efficiently and accurately. We also use the model ensemble approach in order to capture the uncertainty in environment.

% The algorithmic framework maintains two buffers: the first layer experience replay buffer which stores general transition samples (the form of $(s_t, a_t, s_{t+1}, r_{t+1})$) and the second layer experience replay buffer which stores the prioritized transition samples. At each time step $t$, a real experience $(s_t, a_t, s_{t+1}, r_{t+1})$ is collected and stored into the first layer experience replay buffer. Then the priority process starts to prioritize the samples and store them into the second layer experience replay buffer. A hypothetical experience is obtained by first selecting a state $s$ from the experience replay buffer (as shown in Figure \ref{fig:algo-structure}), then selecting an action $a$ according to the current policy, and then querying the model to get the next state $s'$ and reward $r$ to form an experience $(s, a, s', r)$. These hypothetical transitions are combined with real experiences into a single replay buffer to update the training parameters. The updates performed before taking the next action, are called \emph{planning updates}~\cite{sutton2018intro}, as they improve the action-value estimates---and so the policy---using a model. The choice of pairing states with on-policy actions to form hypothetical experiences has been reported to be beneficial~\cite{gu2016continuous,yangchen2018rem,janner2019trustmodel}.

在本小节中，我们将介绍双层经验重放法以及它如何帮助提高强化学习的学习效率。

我们首先根据环境数据训练一个模型，即$\mathcal{D}_{text{env}}$。得到模型后，我们先生成一定量的样本填充到多层批处理中，先取出偏差较大的假设样本进行学习，然后直接优先推断并学习可能存在较大偏差的方向，以弥补代理策略的不足，接着进行常规学习。利用Dyna式RL的思想，我们在策略优化周期中不断将收集到的真实样本传输给模型进行进一步优化，这使得模型能够更高效、更准确地为框架提供模拟样本。我们还使用了模型集合的方法，以捕捉环境中的不确定性。

该算法框架保持两个缓冲区：第一层经验重放缓冲区，存储一般的过渡样本（形式为$(s_t, a_t, s_{t+1}, r_{t+1})$）；第二层经验重放缓冲区，存储优先级的过渡样本。在每个时间步长$t$，一个真实的经验$(s_t, a_t, s_{t+1}, r_{t+1})$被收集并存储到第一层经验回放缓冲区。然后，优先级过程开始对样本进行优先排序，并将其存储到第二层经验重放缓冲区。一个假设的经验是通过首先从经验回放缓冲器中选择一个状态$s$（如图\ref{fig:algo-structure}所示），然后根据当前策略选择一个行动$a$，然后查询模型以获得下一个状态$s'$和奖励$r$，形成一个经验$(s, a, s', r)$。这些假想的过渡与真实的经验结合到一个重放缓冲区，以更新训练参数。在采取下一个行动之前进行的更新，被称为\emph{规划更新}~\cite{sutton2018intro}，因为它们改善了行动价值的估计---因此也改善了政策---使用模型。据报道，选择将状态与政策上的行动配对，形成假设的经验是有益的\cite{gu2016continuous,yangchen2018rem,janner2019trustmodel}。

% Specifically, the scale is controlled by the parameter $\alpha$: the first $\alpha$ scale samples are planned for prioritized experience replay, and the latter $(1-\alpha)$ scale samples are used for regular learning. Then we fill the prioritized samples into the second layer buffer, and then correct the sample distribution through importance sampling in the buffer. Finally, we combine the double layer experience replay for policy update.

% The overview of the algorithm architecture is shown in Figure \ref{fig:algo-structure} and the corresponding pseudo-code is demonstrated in Algorithm \ref{algo:our-method}. As we can see, when interacting with the environment, we collect samples into environment replay buffer $\mathcal{D}_{\mathrm{env}}$, used for training the simulator model of the environment. Then we implement the double layer ER procedure and perform rollouts on the model. The sampled data from the model is filled into a temporary buffer $\mathcal{D}_\mathrm{model}$. Finally, we use samples from $\mathcal{D}_\mathrm{model}$ to optimize the policy $\pi(a|s)$.

具体来说，规模由参数$alpha$控制：第一个$alpha$规模的样本计划用于优先的经验重放，而后一个$(1-alpha)$规模的样本则用于常规学习。然后，我们将优先级的样本填入第二层缓冲区，再通过缓冲区内的重要性抽样来修正样本分布。最后，我们结合双层经验回放进行策略更新。

算法架构的概述见图\ref{fig:algo-structure}，相应的伪代码在Algorithm\ref{algo:our-method}中展示。我们可以看到，当与环境交互时，我们将样本收集到环境重放缓冲区$mathcal{D}_{mathrm{env}}$，用于训练环境的模拟器模型。然后，我们实现双层ER程序，并对模型进行滚屏。来自模型的采样数据被填入一个临时缓冲区$\mathcal{D}_\mathrm{model}$。最后，我们使用$mathcal{D}_\mathrm{model}$的样本来优化策略$pi(a|s)$。

\section{实验设计与分析}

为了理解双层经验重放的潜在好处，我们引入了一个悬崖行走环境（如图\ref{fig:cliff-env}所述）。很明显，有一条最优路径，如图中的红线所示。然而，这条最优路径也是最危险的路径。如果算法在早期阶段效率不高，随时可能不小心从最优路径进入悬崖，造成较大的惩罚，从而影响算法的收敛性，所以具有较好鲁棒性的算法在悬崖行走环境中可以表现得更好。

我们使用政策梯度（PG）算法，并设计了一个变体，在悬崖行走环境中进行对比实验时，会放弃一些滚动样本。如图所示，原始PG算法中的代理人倾向于从远离悬崖的安全路径进行探索，直到它获得足够的信息来慢慢接近最优路径。相比之下，具有双层经验重放的PG算法能够更快地学习，并且能够更有把握地提前接近危险区域，从而更好地收敛到最优路径。统计结果表明，用双层经验重放法训练的策略落入悬崖区的次数较少，在评估过程中表现出更好的稳健性。

表\ref{tab:cliff-stats}显示了在CliffWalking环境下使用单层或双层重放缓冲区的策略梯度的评估统计结果。这些指标是落入悬崖区的概率，以及平均回报率。每次实验的运行由3组种子组成，每次实验需要2000步。可以看出，当使用双层重放缓冲器时，该算法的表现比单层重放缓冲器的情况要好。这一激励性实验给了我们设计算法的灵感。