% !TeX root = ../main.tex

\chapter{总结与展望}\label{chap:summary}

\section{全文总结}

在智能决策学习任务中，算法的样本利用效率和所学策略的安全性往往限制了强化学习的实际应用效果，样本利用效率低下会带来高昂的采样成本，安全性的不足会导致策略做出不稳定甚至错误的决策。针对智能决策学习任务中的这些问题，本文首先提出基于双层缓存的优先经验回放算法（Double-layer Prioritized Experience Replay，DPER）和基于模型集成的筛选规划算法（Model-Based Dropout Planning，MBDP）。DPER算法通过在传统优先经验回放算法（PER）的基础上添加了第二层经验回放池，用于长时缓存全局空间的经验回放样本，达到进一步增加强化学习算法的样本利用效率的效果。而MBDP算法则以对抗的训练方式，设计了能够提升样本利用效率的集成模型筛选模块，以及能够提升算法鲁棒性的模拟数据筛选模块，两者结合而成的MBDP算法能够在保证算法样本利用效率的前提下，对鲁棒性起到大幅的提升作用。

本文首先在CliffWalking和OpenAI提供Mujoco环境下进行了模拟仿真实验，对所设计的DPER算法和MBDP算法进行了实验验证。实验表明，DPER算法中所提出的双层经验回放池设计能够有效地增强策略学习的收敛速度，相比优先经验算法（PER）有着更好的样本效率。而MBDP算法也在Mujoco环境下的验证实验中表现出了优秀的鲁棒性，即使环境发生干扰，也能一致地表现出收敛性能，验证了MBDP算法能够在保证算法样本利用率的同时，起到鲁棒性的提升效果这一结论。

为了进一步验证所提出的算法在真实应用场景中的提升效果，本文以基于模型集成的筛选规划算法为核心，针对自动化温室决策控制任务，设计了能够在温室中根据传感器信息自动控制设备的策略学习算法，并同时在仿真模拟器及真实环境中进行了验证实验。实验表明，算法不仅相比于普通的机器学习方法有着更好的收敛性能，还表现出了超越人类平均水平的决策性能。与此同时，算法在受环境干扰的情况下还表现出优异的稳定性，相比普通的机器学习算法，能够大幅提升自动化温室决策策略的安全性。这一实验结果也进一步地证明了本文所设计的算法能够在强化学习算法中有效提升样本利用效率和安全性。

\section{未来展望}

尽管强化学习算法在自动化决策控制任务中表现出强大的能力，但仍有大量的问题值得更加深入的探索。对于强化学习算法而言，智能体是在与环境反复循环交互的过程中通过不断试错并进行自我判断，对决策选项进行优化更新而最终收敛得到近似最优策略。然而在这一更新过程中，深度神经网络等黑箱模型的加入使得强化学习算法学习得到的策略产生了不可解释性，这也正是强化学习算法缺乏安全性的本质原因。尽管本文中提出的算法能够一定程度上缓解这一问题，但并没有在根源上得以解决。在未来的研究工作中，应该考虑更多开放式结构的白箱模型，使得模型学习和模型规划等环节能够得到可解释性的保障。

此外，本文所研究的样本利用效率问题和安全性问题均是基于单智能体的基本假设，而现实中的应用场景下，往往需要控制多智能体进行分工合作。相比于单智能体强化学习，多智能体背景下会面临更苛刻的样本利用效率问题和安全性问题，在本文的研究基础上，未来还需要将提出的DPER算法和MBDP算法拓展延伸至多智能体环境下，使得它们能够得到更广泛的应用。