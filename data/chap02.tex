% !TeX root = ../main.tex

\chapter{相关理论知识简介}

\section{强化学习基本概念}

在强化学习算法中，将问题背景划分为“决策者”和“环境”两个部分，其中，“决策者”指算法模型本身，“环境”指决策者以外的信息集合。

环境会随时间发生改变，每个时间下都有特定的“状态”，所有可能出现的状态所组成的集合称为“状态空间”，记作 $\mathcal S$ ，而在时刻 $t$ 对应的状态记作 $S_t\in \mathcal S$ 。在每个状态下，决策者可以根据环境状态的信息来采取“行动”，将某个状态 $s$ 下所有可采取的行动组成的集合称为“状态 $s$ 下的行动空间”，记作 $\mathcal{A}(s)$ ，特别地，将 $t$ 时刻 $s$ 状态下的特定行动记作 $A_t\in\mathcal{A}(s)$ 。

决策者采取行动 $A_t$ 后，会对环境产生影响，环境则会因此发生改变，由本时刻的状态 $S_t$ 进入下一时刻的状态 $S_{t+1}$ ，同时对决策者当前时刻的行为 $A_t$ 产生一个评价性反馈并传递给决策者，将这样的评价性反馈称为“奖励”，记作 $R_{t}\in\mathcal{R}\subset\mathbb{R}$ ，其中 $\mathcal{R}$ 为“奖励空间”，表示所有可能的奖励值组成的集合。

决策者收到环境传来的奖励 $R_{t}$ 后，将能得知环境对刚才的行动 $A_t$ 的客观评价，从而根据该信息来调整自己的“决策策略”，并用于进行下一轮行为决策。而决策的调整，则是强化学习里的重要研究对象。简单而言，调整策略的核心思想是要最大化总收益，即将奖励值的累积求和最大化。

这里引入“回报值”的概念，它是总收益值的一个更规范的描述。定义 $t$ 时刻之后的回报值为 $G_t$ ，其表达式为

\begin{equation}
G_t = R_{t+1}+R_{t+2}+R_{t+3}+\cdots
\end{equation}

特别地，强化学习问题可被分为两种类型：片段型和连续型。片段型问题指整体问题能够分解为存在起始状态和终止状态、步骤有限的片段，这些片段均有着相似结构，但不一定完全相同。比如纸牌游戏的一轮牌局就是一个片段，多轮牌局全体构成整体问题。相反，连续型问题则指那些不能分解为子片段的，持续连贯的问题。

在两种问题类型下，回报值 $G_t$ 的公式需做一些调整才能合理。对于片段型问题，从 $t$ 时刻开始，设其所处片段的终止状态对应的时刻为 $T$ ，则应定义

\begin{equation}
G_t = R_{t+1}+R_{t+2}+R_{t+3}+\cdots+R_T
\end{equation}

而对于连续型问题，若简单地将奖励值直接相加，显然会使 $G_t$ 趋于无穷，这将导致无法比较不同行动 $A_t$ 的效果。考虑将 $G_t$ 定义为

\begin{equation}
G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}
\end{equation}

其中 $\gamma(0\leq \gamma < 1)$ 为“削减率”，其直观效果是将观测信息的重点集中在较近时刻的奖励值上。

\section{Markov 决策过程}

强化学习的目标是：学习出能由状态信息决定最佳行动的决策策略\cite{sutton2018reinforcement}。状态信息包含了实时信息和一定的历史信息，而显然历史信息不能太过庞大，否则会严重影响学习效率，此时考虑引入 Markov 性质，将问题的模型表示为 Markov 决策过程\cite{howard1960dynamic}，这样便能将“历史状态信息”通过状态的转移概率分布体现出来\cite{ross1996stochastic}。

\subsection{Markov 性质}

记全部历史信息的联合概率分布为

\begin{equation}
    \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_{t-1},A_{t-1},R_t,S_t,A_t\}
\end{equation}

记环境的状态转移概率分布为

\begin{equation}
    p(s',r|,s,a) = \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_t=s,A_t=a\}
\end{equation}

\begin{definition}
    若有 $p(s',r|s,a) = \mathrm{Pr}\{S_{t+1}=s',R_{t+1}=r|S_0,A_0,R_1,\ldots,S_t,A_t\}$ ，则称这样的问题具有 { Markov 性质}。
\end{definition}

通过引入 Markov 性质，决策者每次进行决策时只需考虑当前状态，因为历史信息也包含在了当前状态之中。

\begin{definition}
    称一个强化学习问题为 Markov 决策过程，当前仅当这个强化学习问题满足 Markov 性质\cite{sutton2018reinforcement}\cite{howard1960dynamic}。
\end{definition}

根据之前的定义，$p(s',r|s,a) =\mathrm{Pr}\left\{S_t=s',R_t=r|S_{t-1}=s,A_{t-1}=a\right\}$ ，且有 $\sum_{s'\in\mathcal{S}}\sum_{r\in\mathcal{R}}p(s',r|s,a)=1$ 。

其中，$p(s',r|s,a)$ 表示决策者在处于状态 $s$ 时做出行动 $a$ 的条件下，进入下一个状态 $s'$ 并收到奖励值反馈 $r$ 的概率分布。

\subsection{状态价值函数与行为价值函数}

强化学习的核心是通过对行动的评价来调整决策策略，需要将行动的评价以及决策策略都定义为函数，才能进行进一步的具体分析。

记概率空间为 $\mathcal P$ ，将策略函数定义为映射：$\pi:\mathcal{S}\times\mathcal{A}\to \mathcal{P}$ 。特别地，条件概率 $\pi(a|s)$ 表示决策者在状态 $s$ 时选择行动 $a$ 的概率分布。

确定了策略 $\pi$ 后，可以定义状态值函数和行动值函数：

\begin{equation}
    \begin{aligned}
        v_{\pi}(s) &=\mathbb{E}_{\pi}\left[G_t|S_t=s\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s\right], \forall s \in \mathcal{S}\\
        q_{\pi}(s,a) &=\mathbb{E}_{\pi}\left[G_t|S_t=s,A_t=a\right]=\mathbb{E}_{\pi}\left[\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\mid S_t=s,A_t=a\right]
    \end{aligned}
\end{equation}

其中，$v_\pi(s)$ 表示从状态 $s$ 开始，决策者之后若完全遵守策略 $\pi$ 来采取行动，所能得到的期望回报值。而 $q_\pi(s,a)$ 表示在状态 $s$ 下，已经采取了某个行动 $a$ ，之后若完全遵守策略 $\pi$ 采取行动，所能得到的期望回报值。

\section{基于 Bellman 最优方程的强化学习算法}

\begin{definition}
    在有限马尔可夫决策过程中，当前状态 $s$ 与未来所有可能状态 $s'$ 满足 Bellman 方程关系，即
    \begin{equation}
        v_{\pi}(s) = \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right]
    \end{equation}
\end{definition}

% \begin{proof}
%     \[
%     \begin{aligned} v_\pi(s) & = \mathbb{E}_\pi[G_t\,|\,S_t=s] \\ &=\mathbb{E}_\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\,|\,S_t=s \right] \\ &= \mathbb{E}_\pi \left [R_{t+1}+\gamma\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_t=s \right] \\ &=\sum_a\pi(a|s)\sum_{s'}\sum_rp(s',r|s,a)\left[r+\gamma \mathbb{E}_\pi \left[\sum_{k=0}^\infty\gamma^kR_{t+k+2}\,|\,S_{t+1}=s' \right] \right] \\ &= \sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\gamma v_\pi(s') \right], \qquad \forall s \in \mathcal S \end{aligned}
%     \]

%     得证。
% \end{proof}

强化学习问题的目标是找到解决问题的最优策略，这一目标的前提是得到最优策略对应的价值函数，才能根据这个价值函数推出最优策略。

\begin{definition}
    在策略空间 $\mathcal{P}$ 中存在且至少存在一个策略优于其他所有策略，称满足这一条件的策略为\textbf{最优策略}，其对应的价值函数 $v_*(s)$ 和 $q_*(s,a)$ 分别称为{最优状态值函数}、\textbf{最优行动值函数}。
\end{definition}

在该定义的基础上，可以得到 Bellman 最优方程\cite{howard1960dynamic}。$v_*$与$q_*$ 作为价值函数时，状态 $s$ 与未来所有可能状态 $s'$ 满足 Bellman 最优方程关系，即
    \begin{equation}\label{eq:v_star}
        v_*(s)=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]
    \end{equation}
    
    \begin{equation}\label{eq:q_star}
    q_*(s,a) = \sum_{s',r}p(s',r \mid s,a)\left[r+\gamma \max_{a'}q_*(s',a') \right]
\end{equation}

% \begin{proof}
%     在最优策略下，显然有

%      \[
%     \begin{aligned}v_*(s) &= \max_{a\in \mathcal A(s)}q_{\pi_*}(s,a) \\ &=\max_a \mathbb{E}_{\pi_*} \left[G_t \mid S_t=s, A_t=a \right] \\ &= \max_a \mathbb{E}_{\pi_*} \left[\sum_{k=0}^\infty \gamma^kR_{t+k+1} \mid S_t=s,A_t=a \right] \\ &= \max_a \mathbb{E}_{\pi_*} \left[R_{t+1}+\gamma \sum_{k=0}^\infty\gamma^kR_{t+k+2}\mid S_t=s,A_t=a \right] \\ &= \max_a \mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1}) \mid S_t=s,A_t=a] \\ &=\max_{a}\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_*(s')]\end{aligned}
%     \]

%     得证。
% \end{proof}

在求得 $v_*,q_*$ 的基础上，即可得到{ 最优策略}，最优策略的定义如下：

\begin{definition}
    显然在行动空间 $\mathcal{A}(\mathcal{S})$ 中存在且至少存在一个行动，其行动价值函数值能取到 $v_*$ 或 $q_*$，记该行动为 $a_*$，如果一个策略只将非零概率分配给这些行动，称这个策略是最优策略。
\end{definition}

至此，已经得到一个最基本的强化学习算法：求解 Bellman 最优方程得到最优价值函数 $v_*$ 和 $q_*$ ，根据最优价值函数确定出最优策略。但由于 Bellman 方程较为复杂，直接求解计算难度较大，所以在实际中并不实用，需要对其进行一定的改进。

% \subsection{迭代法近似求解 Bellman 方程}

% 为了提升求解 Bellman 方程的效率，考虑采用 Jocobi 迭代法\cite{2007numanalysis}。Jacobi 迭代法的收敛条件为

% \begin{definition}\label{the:jacobi}
%     对于方程组 $\boldsymbol{x}_{k+1} = \boldsymbol{Cx}_{k}+\boldsymbol{b}$ ，给定方程组的一个初始近似解 $\boldsymbol{x}_0$ ，由前式迭代产生的序列 $\left\{\boldsymbol{x}_0,\boldsymbol{x}_1,\boldsymbol{x}_2,\cdots,\boldsymbol{x}_k,\cdots\right\}$ 收敛的充分必要条件是矩阵 $\boldsymbol{C}$ 满足条件 $\lim\limits_{k \rightarrow \infty} \boldsymbol{C}^{k}=\boldsymbol{O}$ 。
% \end{definition}

% \begin{definition}\label{the:bellmanjacobi}
%     Bellman 方程可由 Jacobi 迭代法求得收敛解。
% \end{definition}

% \begin{proof}

% 将 Bellman 方程的迭代形式为

% \begin{equation}\label{eq:itebellman}
%     v_{k+1}(s) = \sum_a\pi(a\mid s)\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_k(s')]
% \end{equation}

% 设状态空间 $\mathcal S$ 中有 $n$ 个不同的状态，则可将该 Bellman 迭代式具体展开为

% \begin{equation}
%     \begin{aligned}
%         v_{k+1}(s_1)=&c_{11}v_k(s_1)+c_{12}v_k(s_2)+\cdots+c_{1n}v_k(s_n)+b_1\\
%         v_{k+1}(s_2)= & c_{21}v_k(s_1)+c_{22}v_k(s_2)+\cdots+c_{2n}v_k(s_n)+b_2\\
%         &\vdots\\
%         v_{k+1}(s_n)= & c_{n1}v_k(s_1)+c_{n2}v_k(s_2)+\cdots+c_{nn}v_k(s_n)+b_n\\
%         \end{aligned}
% \end{equation}

% 其中

% \begin{equation}
%     \begin{aligned}
%         c_{ij} &=\sum_a\pi(a\mid s_i)\sum_{r}p(s_j,r \mid s_i,a)\gamma\\
%         &\leq \sum_a\pi(a\mid s_i)\sum_{s_j,r}p(s_j,r \mid s_i,a)\gamma=\gamma
%     \end{aligned}
% \end{equation}

% \begin{equation}
%     b_{i} =\sum_a\pi(a\mid s_i)\sum_{r}p(r\mid s_i,a)r
% \end{equation}

% 记

% \begin{equation}
%     \boldsymbol{C} =
%     \left(\begin{array}{cccc}
%     c_{11} & c_{12} & \cdots & c_{1n} \\
%     c_{21} & c_{22} & \cdots & c_{2n} \\
%     \vdots & \vdots & \ddots & \vdots \\
%     c_{n1} & c_{n2} & \cdots & c_{nn}
%     \end{array}\right)
% \end{equation}

% 则可简记 Bellman 迭代式的矩阵形式为

% \begin{equation}\label{eq:bellmaneq}
%     \boldsymbol{v}_{k+1} =\boldsymbol{Cv}_k+\boldsymbol{b}
% \end{equation}

% 对于 Bellman 迭代式 \ref{eq:bellmaneq} ，由于 $0\leq\gamma<1$ ，显然有

% \begin{equation}
%     \lim_{k\rightarrow \infty} \boldsymbol{C}^k = \lim_{k\rightarrow \infty}\left(\begin{array}{cccc}
%     \gamma & \gamma & \cdots & \gamma \\
%     \gamma & \gamma & \cdots & \gamma \\
%     \vdots & \vdots & \ddots & \vdots \\
%     \gamma & \gamma & \cdots & \gamma
%     \end{array}\right)^k = \boldsymbol{O}
% \end{equation}

% 因此由定理 \ref{the:jacobi} 可知，Bellman 迭代式 \ref{eq:itebellman} 可由 Jacobi 迭代法快速求解，并且解收敛，定理得证。

% \end{proof}

% 根据定理\ref{the:bellmanjacobi}的结论，对于任意强化学习问题，都可以给出一个基于 Bellman 迭代求解的强化学习算法：

% \begin{algorithm}[H]
%     \caption{基于 Bellman 迭代求解的强化学习算法}
%     \begin{algorithmic}[1] %每行显示行号
%         \State 初始化向量 $V(s), \forall s\in \mathcal S$
%         \Repeat
%         \State $\Delta\leftarrow 0$
%         \For{每个 $s\in\mathcal S$}
%         \State $v\leftarrow V(s)$
%         \State $V(s)\leftarrow \max_a\sum_{s',r}p(s',r|s,a)[r+\gamma V(s')]$
%         \State $\Delta \leftarrow \max(\Delta,|v-V(s)|)$
%         \EndFor
%         \Until{ $\Delta<\theta$}
%         \State
%         \State $\pi(s)=\arg\max_a\sum_{s',r}p(s',r|s,a)[r+\gamma V(s')]$
%         \State 输出策略 $\pi$
%     \end{algorithmic}
% \end{algorithm}

\section{动态规划}

由于Bellman方程仅在理论层面上是容易求解的，在实际中，我们需要使用动态规划的思想来使强化学习算法更容易被计算机程序求解。强化学习中动态规划的核心思想体现在将Bellman方程作为更新规则，通过迭代求解的方法来获取近似解，即按照由Bellman方程形式给定的值函数定义求解最优状态值函数（\ref{eq:v_star}）和最优行动值函数（\ref{eq:q_star}）。

对于状态值函数的更新公式

\begin{equation}
\begin{aligned}
v_\pi(s) &\doteq \mathbb{E}_\pi[R_{t+1}+\gamma R_{t+2}+ \gamma^2 R_{t+3}+ \cdots \mid S_t = s] \\ 
&= \mathbb{E}_\pi[R_{t+1}+\gamma v_\pi(S_{t+1}) \mid S_t=s]\\ 
&=\sum_a\pi(a\mid s)\sum_{s',r}p(s',r\mid s,a)[r+\gamma v_\pi(s')]
\end{aligned}
\end{equation}

在已知 $p(s',r|s,a)$ 的条件下，那么上 式可视为一个可求解的线性方程组，但考虑到求解速度受限于状态空间$|\mathcal{S}|$的大小，在高维状态空间的复杂场景下难以求解，而使用动态规划的思想便能有效地通过迭代更新来逐步求解出状态值函数，即通过迭代求解法，找到序列 $\{v_k\}$ 使得 $\lim\limits_{k\to\infty}v_k=v_\pi$。

将状态值函数 $v_\pi$ 的Bellman方程重写为更新规则式：

\begin{equation}
\begin{aligned}v_{k+1}(s) &\doteq \mathbb{E}_\pi[R_{t+1}+\gamma v_k(S_{t+1}) \mid S_t=s] \\ &= \sum_a\pi(a\mid s)\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_k(s')]\end{aligned}
\end{equation}

将方程未知元的系数记为 $c_{ij}$，常数项记为 $b_i$，方程组组可展开为如下形式

\begin{equation}
\begin{aligned}
v_{k+1}(s_1)=&c_{11}v_k(s_1)+c_{12}v_k(s_2)+\cdots+c_{1n}v_k(s_n)+b_1\\
v_{k+1}(s_2)= & c_{21}v_k(s_1)+c_{22}v_k(s_2)+\cdots+c_{2n}v_k(s_n)+b_2\\
&\vdots\\
v_{k+1}(s_n)= & c_{n1}v_k(s_1)+c_{n2}v_k(s_2)+\cdots+c_{nn}v_k(s_n)+b_n\\
\end{aligned}
\end{equation}

记

\begin{equation}
\boldsymbol{C} =
\begin{bmatrix}
c_{11} & c_{12} & \cdots & c_{1n}  \\
c_{21} & c_{22} & \cdots & c_{2n}  \\
\vdots & \vdots & \ddots & \vdots  \\
c_{n1} & c_{n2} & \cdots & c_{nn}
\end{bmatrix}
\end{equation}

\begin{equation}
\boldsymbol{b} =
    \begin{bmatrix}
    b_{1} & b_{2} & \vdots & b_{n} 
    \end{bmatrix}^{\mathrm{T}}
\end{equation}

在矩阵形式下可将Bellman方程的迭代更新规则表示为如下形式：

\begin{equation}\label{eq:bellman-update}
    \boldsymbol{v}_{k+1} =\boldsymbol{Cv}_k+\boldsymbol{b}
\end{equation}

由于

\begin{equation}
\begin{aligned}
        c_{ij} &=\sum_a\pi(a\mid s_i)\sum_{r}p(s_j,r \mid s_i,a)\gamma\\
        &\leq \sum_a\pi(a\mid s_i)\sum_{s_j,r}p(s_j,r \mid s_i,a)\gamma=\gamma
    \end{aligned}
\end{equation}

\begin{equation}
b_{i} =\sum_a\pi(a\mid s_i)\sum_{r}p(r\mid s_i,a)r
\end{equation}

根据定义 $0\leq\gamma<1$，可以计算上述方程的系数矩阵极限为零矩阵：

\begin{equation}
\lim_{k\rightarrow \infty} \boldsymbol{C}^k \leq \lim_{k\rightarrow \infty}\begin{bmatrix}
    \gamma & \gamma & \cdots & \gamma\\
    \gamma & \gamma & \cdots & \gamma\\
    \vdots & \vdots & \ddots & \vdots\\
    \gamma & \gamma & \cdots & \gamma
    \end{bmatrix}^k = \boldsymbol{O}
\end{equation}

即 $\boldsymbol{C}^k\rightarrow\boldsymbol{O}$，根据Jacobi 迭代法的收敛定理，可以证明方程（\ref{eq:bellman-update}）一定可以通过Jacobi迭代法快速求解得到收敛解。在通过动态规划求解得到的价值函数之后，则需考虑如何使用这一价值函数用于优化决策策略$\pi$。具体的做法是将上述的值估计求解算法与策略优化算法相结合，并进行循环优化。策略优化算法的核心思路是按照如下公式选取最优动作$a$替换原有策略中的部分决策：

\begin{equation}
    \begin{aligned}\pi'(s) &= \mathop{\arg\max}\limits_a q_\pi(s, a)\\&=\mathop{\arg\max}\limits_a \mathbb{E}[R_{t+1}+\gamma v_\pi(S_{t+1})\mid S_t=s,A_t=a] \\&= \mathop{\arg\max}\limits_a \sum_{s',r} p(s', r | s, a) (r + \gamma v_\pi(s'))\end{aligned}
\end{equation}

将策略价值函数估计（Policy Evaluation）简记为E步骤，由符号“$\xrightarrow{E}$”表示；策略优化（Policy Improvement）简记为I步骤，由符号“$\xrightarrow{I}$”表示。则上述的循环优化过程可以表示为如下所示的流程

\begin{equation}
\pi_0 \xrightarrow{E} v_{\pi_0} \xrightarrow{I}\pi_1 \xrightarrow{E} v_{\pi_1} \xrightarrow{I} \pi_2 \xrightarrow{E} \cdots \xrightarrow{I} \pi_* \xrightarrow{E} v_*
\end{equation}

在有限Markov决策过程中，状态空间$\mathcal{S}$和行为动作空间$\mathcal{A}$都是有限的，显然可见策略空间也是有限的。而考虑到在策略价值函数估计和策略优化的循环过程中价值函数存在上界且单调递增，由单调收敛定理可知，存在一个步数$N$，当循环次数大于$N$后，所得策略能够近似逼近理想最优策略。


% \section{时序差分学习}

%  强化学习导论（六）- 时序差分学习

% 本章在上一章的基础上，进一步介绍了新的方法：**时序差分学习**（Temporal-Difference learning），简称 TD Learning。

% 时序差分学习可以看作蒙特卡罗（MC）和动态规划（DP）的一种结合：

% - 和 MC 的相似之处在于，TD 方法从实际的经验来获取信息，无需获知环境的全部信息。
% - 和 DP 的相似之处在于，TD 方法能够利用上之前已知的信息来做实时学习，无需等得到完整的收益反馈再进行估值更新。

% 本章同样基于 **GPI 模型**，来介绍基于时序差分学习（TD learning）的算法。

%  6.1 TD Prediction

% 我们先对比看看上一章的 MC 算法（固定$\alpha$）和本章的 TD 算法的核心公式：

% **constant-$\alpha$ MC:**

% $$
% V(S_t)\leftarrow V(S_t)+\alpha\left[G_t-V(S_t)\right]
% $$

% **TD(0) (one-step TD):**

% $$
% V(S_t)\leftarrow V(S_t)+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right]
% $$

% - MC 方法必须等待整个 episode 结束后得到 $G_t$ 才能做一次更新。
% - TD 方法则只需等到这一步结束，利用实时观测到的奖励值 $R_{t+1}$ 和现有估计值 $V(S_{t+1})$ 来进行更新。

% *这里的 TD(0) 方法指**单步 TD 方法**，括号里的 0 改为其他数字后又指其他算法，将会在后面章节介绍。*

% 我们在第二章讲过，这样的更新式可以更广义地写作

% $$
% NewEstimate\leftarrow OldEstimate+StepSize\left[Target-OldEstimate\right]
% $$

% 将括号中的式子看作是一种误差，便可将这样的更新式看作是不断地在消除误差。我们将 TD 方法中的这个误差定义为 **TD error:**

% $$
% \delta_t\doteq R_{t+1}+\gamma V(S_{t+1})-V(S_t)
% $$

%  6.2 Advantages of TD Prediction Methods

%  优点

% - 无须获知环境的具体模型。
% - 通过一种在线的、完全实时的方式来进行增量更新。
% - 如果 episodes 太长，或者是连续型任务，MC 方法将会有很严重的延迟问题，TD 方法能够解决这种问题。

%  收敛性

% 给定策略 $\pi$ ，满足一定条件的情况下，能够证明 TD(0) 方法能确保 $v$ 收敛到 $v_\pi$：

% $$
% \sum_{k=1}^{\infty}\alpha=\infty\qquad \mathrm{and}\qquad\sum_{k=1}^{\infty}\alpha^2<\infty
% $$

% 这个结果是来自随机逼近方面的理论，我们在第二章也提到过。需要注意的是，这只是其收敛的一个必要条件，一些情况下即使不满足，也一样能收敛。

%  效率对比

% - 目前还没能从数学上证明哪个方法（TD \& MC）收敛得更快。
% - 实际情况下，对于随机性的任务，TD 方法通常收敛得比 constant-$\alpha$ MC 方法要快一些。

% **一个例子**:

% % % ![](imgs/RLAI_6/rand-walk.png)

% 问题的背景就是从中间结点 C 出发，若能达到右边的终止态，返回奖励值 1，其余情况均返回奖励值 0。我们用 $P_i$ 来表示当我们处于状态 i 下，最终能到达右终止态的概率，易知

% $$
% \begin{cases}
% P_a=\dfrac{1}{2}P_b+\dfrac{1}{2}\times 0\\
% P_b=\dfrac{1}{2}P_a+\dfrac{1}{2}P_c\\
% P_c=\dfrac{1}{2}P_b+\dfrac{1}{2}P_d\\
% P_d=\dfrac{1}{2}P_c+\dfrac{1}{2}P_e\\
% P_e=\dfrac{1}{2}P_d+\dfrac{1}{2}\times 1\\
% \end{cases}
% $$

% 解上述方程，可得

% $$
% P_a=\frac{1}{6},P_b=\frac{2}{6},P_c=\frac{3}{6},P_d=\frac{4}{6},P_e=\frac{5}{6}
% $$

% 由于到达右终止态的 reward 是 1，那么乘上这个奖励值，最终的期望值其实就等于这些概率

% $$
% V(A)=\frac{1}{6},V(B)=\frac{2}{6},V(C)=\frac{3}{6},V(D)=\frac{4}{6},V(E)=\frac{5}{6}
% $$

% 那么，在这个问题背景下，我们的算法表现如何呢？

% % % ![](imgs/RLAI_6/rand-walk-result.png)

% - 左图描绘了 TD(0) 算法下经过 n 个 episodes 后得到的估计值，可以看出，100 次训练后，估计值已经非常接近真实值了。
% - 右图对比了两种算法在不同 $\alpha$ 取值下的误差收敛曲线。可以看出，至少在该例中，TD 方法确实要优于 MC 方法。

%  6.3 Optimality of TD(0)

%  批量更新（Batch Update）

% $$
% V(S_t)\leftarrow V(S_t)+\alpha\left[G_t-V(S_t)\right]\\
% V(S_t)\leftarrow V(S_t)+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right]
% $$

% 对于上面这样的更新式，我们有时也可通过**批量更新（Batch Update）**来进行更新操作：

% - 每一步 t 我们仍按照原来的步骤做计算，求出增量（也就误差值）。
% - 先不执行更新，而是将增量（误差值）累积起来。
% - 当一整批训练数据都按上述步骤处理完后，再统一将增量更新到目标值上。

%  Certainty-Equivalence Estimate

%  例子

% 给定 8 个 episodes 作为一个 batch：

% $$
% \begin{aligned}
% (A,0,B,0)\qquad&(B,1)&(B,1)\qquad&(B,1)\\
% (B,1)\qquad&(B,1)&(B,1)\qquad&(B,0)
% \end{aligned}
% $$

% 很显然，可以看出 $V(B)=\frac{3}{4}$ ，而 $V(A)$ 则有两种答案，在 TD 方法下 $V(A)=\frac{3}{4}$ ，而对于 MC ，$V(A)=0$ 。为什么会发生这样的情况呢？我们先来按照两种方法模拟计算一下：

%  TD(0) methods

% $$
% V_0(A)=V_0(B)=V(T)=0\\
% \alpha_a=1,\alpha_b=\frac{1}{8},\gamma=1
% $$

% $$
% \begin{aligned}
% (1)&{\begin{cases}
% \delta_1(A)=R_0+V_0(B)-V_0(A)=0+0-0=0\\
% \delta_1(B)=\sum_{i}\left[R_{0i}+V(T)-V_0(B)\right]=6+0-0=6\\
% V_1(A)=V_0(A)+1\times\delta_1(A)=0\\
% V_1(B)=V_0(B)+\frac{1}{8}\delta_1(B)=\frac{3}{4}
% \end{cases}}\\
% (2)&{\begin{cases}
% \delta_2(A)=R_1+V_1(B)-V_1(A)=0+\frac{3}{4}-0=\frac{3}{4}\\
% \delta_2(B)=\sum_{i}\left[R_{1i}+V(T)-V_1(B)\right]=6+0-\frac{8\times3}{4}=0\\
% V_2(A)=V_1(A)+1\times\delta_2(A)=\frac{3}{4}\\
% V_2(B)=V_1(B)+\frac{1}{8}\delta_2(B)=\frac{3}{4}
% \end{cases}}\\
% \vdots\\
% (n)&{\begin{cases}
% \delta_n(A)=0,\delta_n(B)=0\\
% V_n(A)=\frac{3}{4},V_n(B)=\frac{3}{4}
% \end{cases}}
% \end{aligned}
% $$

% % % ![](imgs/RLAI_6/batch-update-td0.png)

%  MC methods

% $$
% V_0(A)=V_0(B)=V(T)=0\\
% \alpha_a=\alpha_b=\frac{1}{8},\gamma=1
% $$

% $$
% \begin{aligned}
% (1)&{\begin{cases}
% \delta_1(A)=G_1(A)-V_0(A)=0-0=0\\
% \delta_1(B)=\sum_{i}\left[G_{0i}-V_0(B)\right]=6-0=6\\
% V_1(A)=V_0(A)+\frac{1}{8}\delta_1(A)=0\\
% V_1(B)=V_0(B)+\frac{1}{8}\delta_1(B)=\frac{3}{4}
% \end{cases}}\\
% (2)&{\begin{cases}
% \delta_2(A)=G_1(A)-V_1(A)=0-0=0\\
% \delta_2(B)=\sum_{i}\left[G_{2i}-V_1(B)\right]=6-6=0\\
% V_1(A)=V_0(A)+\frac{1}{8}\delta_1(A)=0\\
% V_1(B)=V_0(B)+\frac{1}{8}\delta_1(B)=\frac{3}{4}
% \end{cases}}\\
% \vdots\\
% (n)&{\begin{cases}
% \delta_n(A)=0,\delta_n(B)=0\\
% V_n(A)=0,V_n(B)=\frac{3}{4}
% \end{cases}}
% \end{aligned}
% $$

%  确定性等价估计

% - MC 方法本质上是对**训练集**给出了**最小均方误差估计**，在我们这个例子中，A 出现了的 episode 只有一个，而恰好其最终返回值 $G(A)=0$ ，MC 方法基于这段经验就认为 $V(A)=0$ ，这就显得有点「过拟合」了——过于追求训练集上的最小误差，而没有充分考虑综合因素。
% - TD(0) 方法则是给出了**极大似然估计**：$\hat{\theta}=\mathop{\arg\max}\limits_\theta p(\theta|\mathbf{x})$ 。此处的极大似然估计不是某个具体的参数，而是基于观测数据形成的马尔可夫过程**模型**。极大似然估计其实就是**给定数据的最大生成概率的对应估计**，也就是说，前面的这个模型包含了对状态转移概率、奖励值等信息的正确估计，一旦给定正确的信息，它就能作出合理的估计。我们称这样的估计为**确定性等价估计（Certainty-Equivalence Estimate）**。

% 而 TD 方法之所以通常比 MC 收敛得更快，正是因为它是确定性等价估计，更加稳健高效。

% 这三节均是基于 GPI 介绍具体的 TD 算法，由于三种算法本质相近，故将三节合在一起说。

% 前面提到，我们要按照下面的形式来更新目标值

% $$
% Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma\hat{Q}(S_{t+1},A_{t+1})-Q(S_t,A_t)\right]
% $$

% 注意到更新式中我们把 $Q$ 写作 $\hat{Q}$ ，统一表示对 $Q$ 的估计，可以是原有的估计方式 $Q(S_{t+1},A_{t+1})$ ，也可以是其他的估计，在这三节里，不同的估计方法就分别对应了不同的算法，那为什么要这样做呢？

% 我们前面介绍的 TD 算法是在拿旧的 $Q_{old}$ 计算误差：

% $$
% R_{t+1}+\gamma Q_{old}(S_{t+1},A_{t+1})-Q_{old}(S_t,A_t)
% $$

% 但我们本应用 $Q_{new}$ 来计算：

% $$
% R_{t+1}+\gamma Q_{new}(S_{t+1},A_{t+1})-Q_{old}(S_t,A_t)
% $$

% 这种直接采用旧的估计值来更新 target 的方法，稍加分析能够发现其实就是统计学里的**自助抽样法（bootstrap)**——有放回地重新抽样。回到前面的问题，为什么要这样做？在我们重新抽样后，有人认为应按原有方式继续那样计算，也有人认为那样计算不能很好地代表 $Q_{new}$ ，所以，在不同的见解下，对这个估计值 $\hat{Q}$ 做不同的调整，形成了不同的算法。

%  Sarsa 算法

% 若 $\hat{Q}(S_{t+1},A_{t+1})=Q(S_{t+1},A_{t+1})$ ，这时的 TD 算法称为 Sarsa ，其更新式为

% $$
% Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\right]
% $$

% - 由于算法的每次更新需要用到数据组合 $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$ ，故取其字母组成其名字 SARSA ，称为 Sarsa 算法。
% - 显然，Sarsa 是 On-policy 的算法，且和 MC 相似，需要确保各状态都被访问足够多次数才能收敛，因此一般都用于 $\varepsilon$-greedy 或 $\varepsilon$-soft 策略。

% % % ![](imgs/RLAI_6/sarsa.png)

%  Q-learning 算法

% 若 $\hat{Q}(S_{t+1},A_{t+1})=\max\limits_aQ(S_{t+1},a)$ ，这时的 TD 算法称为 Q-learning ，其更新式为

% $$
% Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma  \max\limits_aQ(S_{t+1},a)-Q(S_t,A_t)\right]
% $$

% 易分析知，我们是从所有的 $Q(S_{t+1},a)$ 中直接**选取**了最大值来更新（而不是像前面的 Sarsa 一样还需往前实际走一步 $A_{t+1}$）就能完成更新， 更新之后，我们可以任意采取其他策略来做 exploration actions ，所以 Q-learning 是 Off-policy 方法。

% % % ![](imgs/RLAI_6/q-learning.png)

%  Expected Sarsa 算法

% 若 $\hat{Q}(S_{t+1},A_{t+1})=\mathbb{E}_\pi\left[Q(S_{t+1},A_{t+1})\mid S_{t+1}\right]$ ，这时的 TD 算法称为 Expected Sarsa ，其更新式为

% $$
% \begin{aligned}
% Q(S_t,A_t)&\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma  \mathbb{E}_\pi\left[Q(S_{t+1},A_{t+1})\mid S_{t+1}\right]-Q(S_t,A_t)\right]\\
% &\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})Q(S_{t+1},a)  -Q(S_t,A_t)\right]
% \end{aligned}
% $$

% Expected Sarsa 可以看作是对 Sarsa 的一种改进，它只会略微增加计算上的消耗，但能降低方差，更加稳定。

% 下面是一个具体的实验范例：

% % % ![](imgs/RLAI_6/performance-compare.png)

% - **Asymptotic performance**: 图中的 Asymptotic performance 指 100,000 个 episodes 之后的实验结果均值。
% - **Interim performance**: 图中的 Interim performance 指 100 个 episodes 之后的实验结果均值。

% 从图中可以看出，即使 $\alpha=1$ ，Expected Sarsa 也一样能够收敛（此时其形式很接近 DP ），而 Sarsa 则只能在 $\alpha$ 较小时才有好的表现。

% Expected Sarsa 可以是 On-policy ，也可以是 Off-policy 。

%  6.7 Maximization Bias and Double Learning

% 前面提到的许多算法都含有**最大化**的操作，通过这些最大化操作来逐步构建出最优策略，比如 Q-learning 中有 $\max\limits_a Q(S_{t+1},a)$ 、Sarsa 中也常有 $\varepsilon$-greedy 策略，同样包含最大化操作。这些最大化操作常常会造成显著的正偏差。

% 我们考虑一个例子：

% - 只有一个状态 s 。
% - 许多行动 a 的真实值 $q(s,a)$ 为 0 ，而估计值 $Q(s,a)$ 在 0 附近波动，可正可负。
% - 最大化操作下的估计值，很大概率会是正值，因此产生了正偏差。

% 我们称上面这类的正偏差为 **maximization bias** 。

% 显然我们是需要消除这样的偏差值，下面引入 Double Learning 来解决这一问题。

%  Double Learning

% 首先看个例子

% % % ![](imgs/RLAI_6/max-bias.png)

% - 如上图右上角的示意图所示，每个 episode 均从状态 A 出发，能够选择的行动只有**向左**或者**向右**，对应的奖励值均为 0 。
% - 由于 $R(B)\sim\mathcal N(-0.1,1)$ ，易分析知，最后会有 $V(left)=-0.1, V(right)=0$ ，所以理论上只应该选择向右而不是向左。

% 然而，在刚开始信息量不足时，向左的行动本应预期反馈 -0.1 左右的值，结果由于算法中的「最大化操作」，反而可能反馈了正的奖励值，导致此时模型更倾向于选择向左行动，如上图红线所示。从这个例子可以看出，正偏差带来的影响非常大，即使是很多次训练后，仍然有较大的误差偏离。因此，需要采用 Double Learning 来消除这一问题。

% **Double Learning**:

% - 将样本分划为两个集合，并分别学习出独立的估计，简记作 $Q_1(a), Q_2(a)$ ，两者均是对真实值 $q(a)$ 的估计。
% - 用其中一个估计值来决定最优行动 $A^*=\mathop{\arg\max}\limits_aQ_1(a)$ 。
% - 通过另一个估计值来计算最优行动对应的值函数 $Q_2(A^*)=Q_2(\mathop{\arg\max}\limits_aQ_1(a))$ 。$Q_2(A^*)$ 是无偏估计，这是因为 $\mathbb{E}[Q_2(A^*)]=q(A^*)$ 。
% - 还可以重复一遍上述过程，并替换两个集合，得到另一个无偏估计 $Q_1(\mathop{\arg\max}\limits_aQ_2(a))$ 。

% 这便是 **double learning** 的思想，能够消除 maxmization bias 造成的影响。理解起来很容易，第一次选出的 $A^*$ 可能代入 $Q_1$ 后得到 $Q_1(A^*)=Q_1(\mathop{\arg\max}\limits_aQ_1(a))=\max\limits_aQ_1(a)$ 为正值，但若代入另一个独立的估计函数 $Q_2$ 后，则显然其结果值不会再像前者那样能必然取到最大值。

% % % ![](imgs/RLAI_6/double-q-learning.png)

% 容易分析知，Double learning 只会加倍内存需求，而不会给计算上带来额外的消耗。

%  6.8 Games, Afterstates, and Other Special Cases

% 一些特殊情况下，我们可以在执行完行动后再去更新，比如下图这样的棋局，同一个局面可以由不同的情况达到，有着相同的期望收益。

% % % ![](imgs/RLAI_6/tic.png)

% 我们称这种行动之后统一的状态为 **Afterstates** 。Afterstates 在一些特殊情况（比如上图这类游戏）很有用处，能够简化问题，大幅提升学习效率。

\section{策略梯度优化}

尽管动态规划算法能够理论上学习得到足够接近最优的策略，但直接求解的方法并没有充分发挥深度机器学习的优势。通过将策略函数进行参数化的做法，可以有效地结合机器学习中的梯度上升/下降算法，更为高效地进行策略学习。

首先定义参数化的策略函数：

\begin{equation}
\pi ( a | s , \boldsymbol { \theta } ) = \operatorname { Pr } \left\{ A _ { t } = a | S _ { t } = s , \boldsymbol { \theta } _ { t } = \boldsymbol { \theta } \right\}
\end{equation}

使用参数化决策策略所产生的价值函数，可定义参数化的性能目标函数：

\begin{equation}
    J(\boldsymbol{\theta}) \doteq v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right)
\end{equation}
其中 $v_{\pi_{\boldsymbol{\theta}}}\left(s_{0}\right)$ 是 $\pi_\theta$ 初始状态为$s_0$条件下的真实价值函数 ，其中策略由 $\boldsymbol{\theta}$ 决定。通过对价值函数 $J ( \boldsymbol { \theta } )$ 的梯度进行梯度更新优化，可以实现性能的最优化，即

\begin{equation}
\boldsymbol { \theta } _ { t + 1 } = \boldsymbol { \theta } _ { t } + \alpha \widehat { \nabla J \left( \boldsymbol { \theta } _ { t } \right) }
\end{equation}

其中$\widehat { \nabla J \left( \boldsymbol { \theta } _ { t } \right) } \in \mathbb { R } ^ { d ^ { \prime } }$ 是梯度的估计值，满足$E_\boldsymbol{\theta}\left[\widehat { \nabla J \left( \boldsymbol { \theta } \right) }\right]=\nabla V(\boldsymbol{\theta})$，即其期望值为真实的梯度。

基于以上的定义，可以给出如下定理：

\begin{theorem}
给定状态访问概率分布$\mu(s)$、行动价值函数$q_\pi(s,a)$，性能目标函数的梯度$\nabla J(\boldsymbol{\theta})$与策略函数的梯度$\pi(a | s, \boldsymbol{\theta})$存在如下所示的正比关系：

\begin{equation}
    \nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta})
\end{equation}
\end{theorem}

$$
\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s) \sum_{a} q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta})
$$

这一定理的意义在于，当我们直接优化目标函数的梯度$\nabla J(\boldsymbol{\theta})$时，等价于在对策略函数$\pi$进行梯度优化，即目标函数$J(\boldsymbol{\theta})$的最优化参数$\boldsymbol{\theta_*}$也是策略函数$\pi(a|s,\theta)$的最优化参数。在该定理的保证下，我们可以直接对$J(\boldsymbol{\theta})$进行简单的梯度优化即可求得最优策略。


% \section{基于模型的强化学习算法}

% 基于模型的强化学习是通过与模拟环境的交互来提高样本效率的有力工具，这意味着它很好地解决了强化学习中样本不足的问题\cite{kaelbling1996reinforcement,arulkumaran2017deep}。PILCO算法\cite{deisenroth2011pilco}通过高斯过程回归学习概率模型，可以很好地表示环境的不确定性，提高基于模型的方法在复杂环境中的性能。DeepPILCO方法\cite{gal2016improving}通过引入贝叶斯神经网络（BNN）与神经网络的高容量函数近似器，实现了对更复杂环境的建模\cite{blundell2015weight，mackay1995bayesian}。进一步引入了模型集合方法 \cite{rajeswaran2016epopt,kurutach2018model}来全面捕捉环境中的不确定性，这可以增强模型的解释力，提高学习到的策略的鲁棒性 \cite{Chua2018DeepModels,malik2019calibrated}。在本文中，我们采用了模型集合的方法，并与我们设计的辍学机制相结合，以实现性能的可控性和稳健性。

% 通过将模拟模型模块化集成到无模型方法中，可以显著提高原始算法的采样效率，例如，使用模型来增强数据 （\cite{sutton1990integrated,racaniere2017imagination,weber2017imagination}；将模型作为规划器，通过推出模拟来提高状态值的估计 （\cite{atkeson1997comparison,feinberg2018model}。然而，所有这些基于模型的方法都存在模型偏差，因此性能会随着模型误差的增加而降低，导致决策政策的不良。STEVE方法\cite{buckman2018sample}试图通过动态插值rollouts的长度来解决这个问题，以获得更好的性能。MBPO method \cite{janner2019trust}从真实状态出发，在规划中只进行短长度的滚屏，可以有效避免累积的错误。在本文中，我们在MBPO中采用了这种技术，以减少滚动误差。

