% !TeX root = ../main.tex

\chapter{绪论}

本章主要介绍本课题的研究背景与意义、研究现状以及本文的主要研究内容，最后将对本文的组织结构进行阐述。

\section{研究背景与意义}

\subsection{强化学习}

强化学习是机器学习中的一个领域，基于环境状态学习如何决策才能使得收益最大化。学习者不会被告知应该采取什么动作，而是必须通过尝试去发现哪些动作会产生高价值的收益 \cite{sutton2018reinforcement}。

强化学习与机器学习领域中的监督学习和无监督学习不同，监督学习是从外部监督者提供的带标注训练集中进行学习（任务驱动型），无监督学习是一个典型的寻找未标注数据中隐含结构的过程（数据驱动型）。

作为与两者并列的第三种机器学习范式，强化学习主要通过“探索”与“利用”之间的折中权衡来不断学习，智能体必须利用已有的经验来获取收益，同时也要进行探索，从错误中学习，获得更好的决策策略 \cite{kaelbling1996reinforcement}。

在强化学习中，主要通过智能体和环境这两个对象进行交互 \cite{tan1993multi}。智能体可以感知环境的状态，并根据反馈的奖励学习选择一个合适的动作，来最大化长期总收益。而环境会接收智能体执行的一系列动作，对这一系列动作进行评价并转换为一种可量化的信号反馈给智能体。

智能体通过强化学习，可以知道自己在什么状态下，应该采取什么样的动作使得自身获得最大奖励。由于智能体与环境的交互方式与人类与环境的交互方式类似，可以认为强化学习是一套通用的学习框架，可用来解决通用人工智能的问题。因此强化学习也被称为通用人工智能的机器学习方法。 \cite{shoham2003multi}

除了智能体和环境之外，强化学习系统还有四个核心要素：策略、奖励函数、价值函数和环境模型 \cite{szepesvari2010algorithms}。

策略定义了智能体在特定时间的行为方式，是环境状态到动作的映射。奖励函数定义了强化学习问题中的目标，在每一步中，环境向智能体发送一个称为收益的标量数值。

价值函数表示了从长远的角度估计状态的好坏程度，一个状态的价值是一个智能体从这个状态开始，对将来累积的总收益的期望。

环境模型是一种对环境的反应模式的模拟，它允许对外部环境的行为进行推断，环境模型是可选的，根据环境模型的存在与否，主要可以将强化学习算法分为无模型强化学习和基于模型的强化学习这两大类别，相比传统的无模型强化学习，基于模型的强化学习有着收敛速度快，学习效率高等等诸多优点 \cite{polydoros2017survey}。

可以看出，强化学习就是不断地根据环境的反馈信息进行试错学习，进而调整优化自身的状态信息，其目的是为了找到最优策略、或者找到最大奖励的过程。

强化学习是一种对目标导向的学习与决策问题进行理解和自动化处理的计算方法。它强调智能体通过与环境的直接互动来学习，而不需要可效仿的监督信号或对周围环境的完全建模，因而与其他的计算方法相比具有不同的范式。目前，强化学习在包括游戏，广告，推荐，对话系统，机器人等多个领域均展开了广泛的应用。但由于强化学习的样本效率瓶颈和安全性问题，在实际应用场景下仍然面临着重重困难 \cite{kober2013reinforcement}。

本课题旨在解决强化学习中的样本效率瓶颈和安全性问题，对模型学习和模型规划两个主要层面进行深入剖析，尝试提出环境模型的集成与筛选、环境模型的数据生成与筛选，用于改进基于模型的强化学习算法，既能有效发挥环境模型对样本利用效率的提升，也能通过所设计的筛选机制增强强化学习策略的鲁棒性和抗干扰能力，从而实现策略收益性能和安全性的平衡。

\subsection{强化学习的样本利用效率}

近年来强化学习技术被广泛应用于智能决策策略的训练，在虚拟场景获得了巨大进展，例如DeepMind公司开发的AlphaGo围棋AI \cite{chen2016evolution}。而在真实场景下，由于强化学习算法需要实时采集环境数据、发送策略指令，大量与传感器、控制器等硬件设备进行数据交互传输，而这些硬件设备在各种工作场景中效率低下，远不如计算机处理器芯片的高效性能，导致强化学习算法会受限于这些硬件设备的工作效率。

此外，由于所学的决策策略需要大量操控硬件设备，对其损耗较大，导致实际部署的成本高昂。这些因素成为制约强化学习算法应用于真实场景的瓶颈 \cite{arulkumaran2017deep}，也是为什么近些年强化学习在仿真环境中取得了令人瞩目的成就，但却没能在现实问题取得大的突破。

传统的无模型强化学习在训练阶段往往需要和任务环境进行上千万次的交互和数据采样才能收敛学习出决策策略 \cite{degris2012model}，这样的样本数量级在虚拟环境下是完全可行的，而在现实问题中，收集上千万条交互数据的成本是难以想象的，因此传统的无模型强化学习算法极难应用在现实任务场景中。

在众多尝试解决强化学习中的数据依赖的研究方法中，基于模型的强化学习是最有希望的方向之一 \cite{moerland2020model}。相对于无模型强化学习，其最主要区别是引入了对环境的建模，即通过监督训练来得到一个环境模型，其数据是算法和环境的实际交互数据$(s_t,a_t,r_t,s_{t+1},a_{t+1},r_{t+1},\ldots)$。通过训练得到的环境模型$\mathcal{M}$，本课题可以在给定$(s_t,a_t)$的情况下预测下一个状态$s_{t+1}=\mathcal{M}(s_t,a_t)$ 。基于模型的强化学习主要可以根据如何使用所学的环境模型而分为3个类别：

\begin{enumerate}
    \item 作为新的数据源：通过与环境模型交互产生数据，作为额外的训练数据源来补充算法的训练。
    \item 增加决策的上下文信息：在进行状态价值的估计时，通过与环境模型进行交互，交互过程中的上下文信息提供给算法来帮助其决策。
    \item 增加状态价值估计的准确性：在进行状态价值估计时候，基于环境模型来展开一定步数，然后辅助无模型强化学习算法，给出一个更准确的状态价值估计，加速算法收敛。
\end{enumerate}

\subsection{强化学习的安全性}

在强化学习任务中，算法的主要目标是使长期收益最大化，虽然能使收益的期望值最大化，但期望的最大值并不意味着整体意义上的稳定性，即使个别情况较小的低收益，也对整体的期望影响不大，因此在最大化期望收益时，这些偶尔发生的低收益状态会被忽视掉，往往容易导致策略过于追求收益，却忽视整体上的安全性和稳定性 \cite{garcia2015comprehensive}。

尽管在普通的任务或者仿真环境中，一些偶尔发生的决策失误并不会带来太大的损失，使得算法常常忽视这些场景，但若要部署到真实环境中，例如在昂贵的机器人平台，一些不稳定的策略容易使机器人受损 \cite{kormushev2010robot};在农业自动化任务中，一般的强化学习算法策略常常导致农作物突发坏死 \cite{bu2019smart}；而在自动驾驶任务中，未经安全性优化的策略有可能导致危险的事故 \cite{sallab2017deep}。

作为一种缺乏可解释性的决策算法，强化学习训练得到的“黑箱”策略很难再加入人类的先验知识作为指导。由于其不可靠性，强化学习一直不被认可用于真实环境的应用中。但最近一些研究强化学习的安全性的工作为这一难题提供了突破口，主要可以分为3个类别 \cite{munos2016safe}：

\begin{enumerate}
    \item 降低训练方差：过高的方差往往意味着策略的不稳定性，导致在决策使做出一些意料之外的危险动作。在一些工作中，通过将收益方差作为风险指标进行优化，从而提升策略的安全性；
    \item 关注最坏情况：与上面类似地，通过增加对最坏情况的关注度，着重训练这样的高风险状态，使得策略能够合理地处理这些极端情况；
    \item 主动惩罚错误：通过将错误状态出现的次数或概率作为惩罚项进行训练，最终所得策略能够一定程度上规避这些高风险状态，同样提升了安全性。
\end{enumerate}

\section{国内外研究现状}


\subsection{基于模型的强化学习基本框架}

基于模型的强化学习基本框架最早在1991年提出 \cite{Sutton1991DynaReacting}，它实际上是一种模块化的思路，可以应用到现有的各种无模型强化学习算法算法中（例如PPO ，TD3，SAC等算法 \cite{schulman2017proximal,haarnoja2018soft}）。基于模型的强化学习旨在高效的利用经验数据，提高学习效率以及数据利用效率。

基于模型的强化学习算法融合了规化、决策和学习的方法，在传统的无模型强化学习方法基础上，增加了模型学习$\longrightarrow$模型$\longrightarrow$模型规划的过程 \cite{lin1992self}，通过经验数据学习得到一个环境的模型，然后使用模型进行模拟规划，生成模拟数据辅助强化学习进行策略优化更新。

\begin{figure}[tbh]
\centering
\includegraphics[width=0.66\textwidth]{figures/Dyna.png}
\caption{基于模型的强化学习基本框架。}
\label{fig:dyna-structure}
\end{figure}

\subsection{模型学习的优化}

在学得模型之后，最朴素的用法是，将真实环境数据和模型生成数据 混合，然后执行无模型强化学习方法，这一过程可视作一种数据增广的优化方法。更进阶的模型用法是，使用模型进行上下文信息的辅助计算，从而更高效地加速强化学习算法的收敛速度。

虽然基于模型的强化学习算法有较好的样本效率，但由于模型误差，存在一定的局限性，使用这样有偏差的环境模型去预测，进而会带来更大的误差 \cite{zambaldi2018deep}。

在PILCO方法中 \cite{deisenroth2011pilco}，通过学习环境的概率模型，从而实现对复杂环境的不确定性捕捉。具体地，PILCO方法使用了高斯过程回归模型来对环境建模 \cite{quinonero2005unifying}，得到环境的概率模型$\mathcal{N}\left(\mu(s_t,a_t), \sigma(s_t,a_t)\right)$，基于这一概率模型，得到期望收益
\begin{equation}
    J^\pi=\mathbb{E}_{s_{t+1}}\left[V\left(s_{t+1}\right)\right]=\int V\left(s_{t+1}\right) \mathcal{N}\left(s_{t+1} \mid \mu_t, \sigma_t\right) \mathrm{d} s_{t+1}
\end{equation}
进而基于梯度$\dfrac{\mathrm{d}J}{\mathrm{d}\theta}$进行策略优化，得到最优参数，使得$\theta^* = {\arg\min}_\theta J^{\pi_\theta}$，最终训练得到最优策略$\pi^*=\pi_{\theta^*}$ 。\cite{peters2006policy}

不确定性环境模型将模型的误差以概率分布的形式展现出来，便能在模型的状态预测环节多次采样得到服从概率分布的预测状态，相比于传统的确定性预测模型，能够让强化学习策略更新环节接收到更多样化的样本输入，从而一定程度上抵消模型偏差带来的影响。但由于高斯过程回归并不适合对复杂的高维度环境进行建模，在后续的PETS方法中，提出使用贝叶斯神经网络（BNN）来对复杂环境进行不确定性建模 \cite{Blundell2015WeightNetworks}，从而解决了这一问题。如图\ref{fig:BNN}所示，贝叶斯神经网络的权重不再是固定的权值，而是一个概率分布，神经元间的权值将从这些概率分布中采样得到。

\begin{figure}[tbh]
\centering
\includegraphics[width=0.9\textwidth]{figures/BNN.png}
\caption{贝叶斯神经网络与普通神经网络的区别。}
\label{fig:BNN}
\end{figure}

\subsection{模型规划的优化}

基于模型的强化学习另一大核心环节便是模型规划 \cite{walsh2010integrating}，即使构建出一个准确的环境模型，如果不能合理地利用模型辅助强化学习，对真实样本的利用效率也依然无法得到提升。

在MVE方法中 \cite{feinberg2018model}，基于已构建的模型往后展开了$H$步的模拟。由于模型在当前时刻的未来较近步数有着较好的精准度，因此当$H$控制在一个合理范围下时，这$H$步的模拟累积误差相对传统的值函数估计要小很多，将这$H$步模拟替换掉值函数估计，便得到
\begin{equation}
    \hat{V}_{H}\left(s_{0}\right)=\sum_{t=0}^{H-1} \gamma^{t} \hat{r}_{t}+\gamma^{H} \hat{V}\left(\hat{s}_{H}\right)
\end{equation}
MVE将传统无模型强化学习中的值函数学习方法引入模型规划来辅助学习，采用了固定深度的模拟与对剩余部分的估计的结合，实现了模型规划的优化。

在Re-Plan方法中 \cite{williams2017information}，每次规划决策得到行动指令，并且基于该指令实际执行行动后，发现真实情况下的转移状态与之前模型规划得到的推理状态之间存在误差，该方法将真实状态进行储存，用作模型预测的修正，从而在下一次规划时，确保模型所预测的有误差状态能被存储记录修正，该方法可以有效地解决模型预测偏差的问题。

而在MBPO方法中 \cite{janner2019trust}，证明了当规划长度控制在一个可控长度$k$以内，即

\begin{equation}
k^{*}=\underset{k}{\operatorname{argmin}}\left[\frac{\gamma^{k+1} \epsilon_{\pi}}{(1-\gamma)^{2}}+\frac{\gamma^{k} \epsilon_{\pi}}{(1-\gamma)}+\frac{k}{1-\gamma}\left(\epsilon_{m^{\prime}}\right)\right]>0
\end{equation}

此时，模型的误差始终可以保障在一个误差界限以内，从而在可控的预测误差下，能够最大程度地进行充分的模型规划预测。


\section{本文的主要研究内容}

\section{本文的组织结构}